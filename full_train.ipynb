{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import random, string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# try:\n",
    "#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "#     print('Device:', tpu.master())\n",
    "#     tf.config.experimental_connect_to_cluster(tpu)\n",
    "#     tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "# except:\n",
    "#     strategy = tf.distribute.get_strategy()\n",
    "# print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# GCS_PATH = KaggleDatasets().get_gcs_path()\n",
    "# BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 reasons the 2016 election feels so personal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasco police shot mexican migrant from behind,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
       "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
       "2  What do you call a turtle without its shell? d...   True\n",
       "3      5 reasons the 2016 election feels so personal  False\n",
       "4  Pasco police shot mexican migrant from behind,...  False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 -> False, 1 -> True (for humor)\n",
    "le = LabelEncoder()\n",
    "data[\"humor\"] = le.fit_transform(data[\"humor\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.text.values\n",
    "y = data.humor.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.983325"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lens = [len(sentence.split()) for sentence in X]\n",
    "avg_sent_lens = np.mean(sent_lens)\n",
    "avg_sent_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq_length = int(np.percentile(sent_lens, 95))\n",
    "output_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 65000\n",
    "\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=output_seq_length\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chars(text):\n",
    "    return \" \".join(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "humor_text_vocab = text_vectorizer.get_vocabulary()\n",
    "\n",
    "token_embed = Embedding(\n",
    "    input_dim=len(humor_text_vocab),\n",
    "    output_dim=128,\n",
    "    mask_zero=True,\n",
    "    name=\"token_embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_hub_embedding_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "    trainable=False,\n",
    "    name=\"universal_sentence_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J o e   b i d e n   r u l e s   o u t   2 0 2 0   b i d :   ' g u y s ,   i ' m   n o t   r u n n i n g '\n",
      "CPU times: user 467 ms, sys: 30.5 ms, total: 497 ms\n",
      "Wall time: 505 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_chars = [split_chars(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.470575"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average character length per sentence\n",
    "char_lens = [len(sentence) for sentence in X]\n",
    "mean_char_len = np.mean(char_lens)\n",
    "mean_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # alphabet + space + OOV token\n",
    "\n",
    "char_vectorizer = TextVectorization(\n",
    "    max_tokens=NUM_CHAR_TOKENS,\n",
    "    output_sequence_length=output_seq_char_len,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    name=\"char_vectorizer\"\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(X_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embed = Embedding(\n",
    "    input_dim=NUM_CHAR_TOKENS,\n",
    "    output_dim=25,\n",
    "    mask_zero=False,\n",
    "    name=\"char_embed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_token_and_char_embeddings\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " token_input (InputLayer)       [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " char_vectorizer (TextVectoriza  (None, 92)          0           ['char_input[0][0]']             \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_input[0][0]']            \n",
      " rasLayer)                                                                                        \n",
      "                                                                                                  \n",
      " char_embed (Embedding)         (None, 92, 25)       1750        ['char_vectorizer[1][0]']        \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          65664       ['universal_sentence_encoder[1][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 50)          10200       ['char_embed[1][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_char_hybrid (Concatenate  (None, 178)         0           ['dense_3[0][0]',                \n",
      " )                                                                'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 178)          0           ['token_char_hybrid[0][0]']      \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 200)          35800       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 200)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            201         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,911,439\n",
      "Trainable params: 113,615\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. Token inputs/model\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(token_inputs, token_output)\n",
    "\n",
    "# 2. Char inputs/model\n",
    "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings)\n",
    "char_model = tf.keras.Model(char_inputs, char_bi_lstm)\n",
    "\n",
    "# 3. Concat\n",
    "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")(\n",
    "    [token_model.output, char_model.output]\n",
    ")\n",
    "\n",
    "# 4. Dropout layers\n",
    "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
    "combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout)\n",
    "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
    "output_layer = layers.Dense(1, activation=\"sigmoid\")(final_dropout)\n",
    "\n",
    "# 5. Create model\n",
    "model_4 = tf.keras.Model(\n",
    "    [token_model.input, char_model.input],\n",
    "    output_layer,\n",
    "    name=\"model_4_token_and_char_embeddings\"\n",
    ")\n",
    "\n",
    "model_4.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.AUC()\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_char_token_data = tf.data.Dataset.from_tensor_slices((X, X_chars))\n",
    "train_char_token_labels = tf.data.Dataset.from_tensor_slices(y)\n",
    "train_char_token_ds = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)).batch(32).prefetch(AUTOTUNE)\n",
    "\n",
    "train_char_token_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6250/6250 [==============================] - 238s 37ms/step - loss: 0.1614 - accuracy: 0.9376 - precision_1: 0.9414 - recall_1: 0.9334 - auc_1: 0.9836\n",
      "Epoch 2/20\n",
      "6250/6250 [==============================] - 245s 39ms/step - loss: 0.1365 - accuracy: 0.9486 - precision_1: 0.9514 - recall_1: 0.9456 - auc_1: 0.9880\n",
      "Epoch 3/20\n",
      "6250/6250 [==============================] - 250s 40ms/step - loss: 0.1272 - accuracy: 0.9523 - precision_1: 0.9547 - recall_1: 0.9496 - auc_1: 0.9895\n",
      "Epoch 4/20\n",
      "6250/6250 [==============================] - 251s 40ms/step - loss: 0.1212 - accuracy: 0.9544 - precision_1: 0.9575 - recall_1: 0.9511 - auc_1: 0.9904\n",
      "Epoch 5/20\n",
      "6250/6250 [==============================] - 248s 40ms/step - loss: 0.1167 - accuracy: 0.9564 - precision_1: 0.9593 - recall_1: 0.9533 - auc_1: 0.9910\n",
      "Epoch 6/20\n",
      "6250/6250 [==============================] - 248s 40ms/step - loss: 0.1133 - accuracy: 0.9578 - precision_1: 0.9600 - recall_1: 0.9554 - auc_1: 0.9916\n",
      "Epoch 7/20\n",
      "6250/6250 [==============================] - 241s 39ms/step - loss: 0.1096 - accuracy: 0.9588 - precision_1: 0.9614 - recall_1: 0.9560 - auc_1: 0.9921\n",
      "Epoch 8/20\n",
      "6250/6250 [==============================] - 230s 37ms/step - loss: 0.1066 - accuracy: 0.9602 - precision_1: 0.9628 - recall_1: 0.9573 - auc_1: 0.9925\n",
      "Epoch 9/20\n",
      "6250/6250 [==============================] - 231s 37ms/step - loss: 0.1042 - accuracy: 0.9610 - precision_1: 0.9636 - recall_1: 0.9582 - auc_1: 0.9928\n",
      "Epoch 10/20\n",
      "6250/6250 [==============================] - 230s 37ms/step - loss: 0.1016 - accuracy: 0.9621 - precision_1: 0.9645 - recall_1: 0.9596 - auc_1: 0.9931\n",
      "Epoch 11/20\n",
      "6250/6250 [==============================] - 233s 37ms/step - loss: 0.1004 - accuracy: 0.9627 - precision_1: 0.9649 - recall_1: 0.9604 - auc_1: 0.9933\n",
      "Epoch 12/20\n",
      "6250/6250 [==============================] - 219s 35ms/step - loss: 0.0976 - accuracy: 0.9638 - precision_1: 0.9661 - recall_1: 0.9612 - auc_1: 0.9935\n",
      "Epoch 13/20\n",
      "6250/6250 [==============================] - 184s 29ms/step - loss: 0.0966 - accuracy: 0.9641 - precision_1: 0.9662 - recall_1: 0.9618 - auc_1: 0.9938\n",
      "Epoch 14/20\n",
      "6250/6250 [==============================] - 185s 30ms/step - loss: 0.0945 - accuracy: 0.9650 - precision_1: 0.9672 - recall_1: 0.9625 - auc_1: 0.9939\n",
      "Epoch 15/20\n",
      "6250/6250 [==============================] - 188s 30ms/step - loss: 0.0930 - accuracy: 0.9655 - precision_1: 0.9677 - recall_1: 0.9633 - auc_1: 0.9941\n",
      "Epoch 16/20\n",
      "6250/6250 [==============================] - 193s 31ms/step - loss: 0.0919 - accuracy: 0.9657 - precision_1: 0.9679 - recall_1: 0.9633 - auc_1: 0.9943\n",
      "Epoch 17/20\n",
      "6250/6250 [==============================] - 195s 31ms/step - loss: 0.0903 - accuracy: 0.9667 - precision_1: 0.9686 - recall_1: 0.9648 - auc_1: 0.9944\n",
      "Epoch 18/20\n",
      "6250/6250 [==============================] - 202s 32ms/step - loss: 0.0885 - accuracy: 0.9672 - precision_1: 0.9691 - recall_1: 0.9652 - auc_1: 0.9946\n",
      "Epoch 19/20\n",
      "6250/6250 [==============================] - 209s 33ms/step - loss: 0.0873 - accuracy: 0.9676 - precision_1: 0.9693 - recall_1: 0.9657 - auc_1: 0.9948\n",
      "Epoch 20/20\n",
      "6250/6250 [==============================] - 213s 34ms/step - loss: 0.0864 - accuracy: 0.9678 - precision_1: 0.9697 - recall_1: 0.9657 - auc_1: 0.9949\n"
     ]
    }
   ],
   "source": [
    "model_4_history = model_4.fit(\n",
    "    train_char_token_ds,\n",
    "    steps_per_epoch=int(len(train_char_token_ds)*1),\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6250/6250 [==============================] - 227s 36ms/step - loss: 0.0856 - accuracy: 0.9680 - precision_1: 0.9702 - recall_1: 0.9657 - auc_1: 0.9950\n",
      "Epoch 2/15\n",
      "6250/6250 [==============================] - 230s 37ms/step - loss: 0.0843 - accuracy: 0.9687 - precision_1: 0.9701 - recall_1: 0.9672 - auc_1: 0.9950\n",
      "Epoch 3/15\n",
      "6250/6250 [==============================] - 232s 37ms/step - loss: 0.0829 - accuracy: 0.9689 - precision_1: 0.9703 - recall_1: 0.9673 - auc_1: 0.9953\n",
      "Epoch 4/15\n",
      "6250/6250 [==============================] - 231s 37ms/step - loss: 0.0820 - accuracy: 0.9696 - precision_1: 0.9719 - recall_1: 0.9672 - auc_1: 0.9953\n",
      "Epoch 5/15\n",
      "6250/6250 [==============================] - 231s 37ms/step - loss: 0.0805 - accuracy: 0.9697 - precision_1: 0.9717 - recall_1: 0.9675 - auc_1: 0.9955\n",
      "Epoch 6/15\n",
      "6250/6250 [==============================] - 202s 32ms/step - loss: 0.0788 - accuracy: 0.9705 - precision_1: 0.9720 - recall_1: 0.9689 - auc_1: 0.9956\n",
      "Epoch 7/15\n",
      "6250/6250 [==============================] - 187s 30ms/step - loss: 0.0787 - accuracy: 0.9708 - precision_1: 0.9724 - recall_1: 0.9691 - auc_1: 0.9956\n",
      "Epoch 8/15\n",
      "6250/6250 [==============================] - 188s 30ms/step - loss: 0.0791 - accuracy: 0.9702 - precision_1: 0.9714 - recall_1: 0.9689 - auc_1: 0.9957\n",
      "Epoch 9/15\n",
      "6250/6250 [==============================] - 220s 35ms/step - loss: 0.0775 - accuracy: 0.9711 - precision_1: 0.9725 - recall_1: 0.9696 - auc_1: 0.9958\n",
      "Epoch 10/15\n",
      "6250/6250 [==============================] - 230s 37ms/step - loss: 0.0753 - accuracy: 0.9718 - precision_1: 0.9733 - recall_1: 0.9703 - auc_1: 0.9960\n",
      "Epoch 11/15\n",
      "6250/6250 [==============================] - 229s 37ms/step - loss: 0.0761 - accuracy: 0.9717 - precision_1: 0.9733 - recall_1: 0.9699 - auc_1: 0.9959\n",
      "Epoch 12/15\n",
      "6250/6250 [==============================] - 228s 37ms/step - loss: 0.0759 - accuracy: 0.9713 - precision_1: 0.9728 - recall_1: 0.9697 - auc_1: 0.9960\n",
      "Epoch 13/15\n",
      "6250/6250 [==============================] - 232s 37ms/step - loss: 0.0750 - accuracy: 0.9720 - precision_1: 0.9734 - recall_1: 0.9706 - auc_1: 0.9961\n",
      "Epoch 14/15\n",
      "6250/6250 [==============================] - 219s 35ms/step - loss: 0.0735 - accuracy: 0.9725 - precision_1: 0.9736 - recall_1: 0.9714 - auc_1: 0.9962\n",
      "Epoch 15/15\n",
      "6250/6250 [==============================] - 207s 33ms/step - loss: 0.0731 - accuracy: 0.9727 - precision_1: 0.9736 - recall_1: 0.9718 - auc_1: 0.9962\n"
     ]
    }
   ],
   "source": [
    "model_4_history = model_4.fit(\n",
    "    train_char_token_ds,\n",
    "    steps_per_epoch=int(len(train_char_token_ds)*1),\n",
    "    epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: final_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: final_model/assets\n"
     ]
    }
   ],
   "source": [
    "model_4.save(\"final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/roderikmogot/humor-prediction/final_model.zip'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"final_model\", 'zip', \"final_model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d450dd2713cdb92d0c156bada70577c9fb5b0d8051aa225f01b0863d071ac6f8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
